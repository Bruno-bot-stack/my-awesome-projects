{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5263da04",
   "metadata": {},
   "source": [
    "## Part 2.1: Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a490cfc",
   "metadata": {},
   "source": [
    "Python has many libraries available to help you parallelise your scripts across the cores of a single multicore computer. The established option is the multiprocessing library. You can import multiprocessing by typing into ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03f8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55262e",
   "metadata": {},
   "source": [
    "You can read the documentation for this module by typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff15c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package multiprocessing:\n",
      "\n",
      "NAME\n",
      "    multiprocessing\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/multiprocessing\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "DESCRIPTION\n",
      "    # Package analogous to 'threading.py' but using processes\n",
      "    #\n",
      "    # multiprocessing/__init__.py\n",
      "    #\n",
      "    # This package is intended to duplicate the functionality (and much of\n",
      "    # the API) of threading.py but uses processes instead of threads.  A\n",
      "    # subpackage 'multiprocessing.dummy' has the same API but is a simple\n",
      "    # wrapper for 'threading'.\n",
      "    #\n",
      "    # Copyright (c) 2006-2008, R Oudkerk\n",
      "    # Licensed to PSF under a Contributor Agreement.\n",
      "    #\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    connection\n",
      "    context\n",
      "    dummy (package)\n",
      "    forkserver\n",
      "    heap\n",
      "    managers\n",
      "    pool\n",
      "    popen_fork\n",
      "    popen_forkserver\n",
      "    popen_spawn_posix\n",
      "    popen_spawn_win32\n",
      "    process\n",
      "    queues\n",
      "    reduction\n",
      "    resource_sharer\n",
      "    resource_tracker\n",
      "    shared_memory\n",
      "    sharedctypes\n",
      "    spawn\n",
      "    synchronize\n",
      "    util\n",
      "\n",
      "SUBMODULES\n",
      "    reducer\n",
      "\n",
      "CLASSES\n",
      "    builtins.Exception(builtins.BaseException)\n",
      "        multiprocessing.context.ProcessError\n",
      "            multiprocessing.context.AuthenticationError\n",
      "            multiprocessing.context.BufferTooShort\n",
      "            multiprocessing.context.TimeoutError\n",
      "    multiprocessing.process.BaseProcess(builtins.object)\n",
      "        multiprocessing.context.Process\n",
      "    \n",
      "    class AuthenticationError(ProcessError)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AuthenticationError\n",
      "     |      ProcessError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors inherited from ProcessError:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class BufferTooShort(ProcessError)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BufferTooShort\n",
      "     |      ProcessError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors inherited from ProcessError:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class Process(multiprocessing.process.BaseProcess)\n",
      "     |  Process(group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)\n",
      "     |  \n",
      "     |  Process objects represent activity that is run in a separate process\n",
      "     |  \n",
      "     |  The class is analogous to `threading.Thread`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Process\n",
      "     |      multiprocessing.process.BaseProcess\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from multiprocessing.process.BaseProcess:\n",
      "     |  \n",
      "     |  __init__(self, group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Close the Process object.\n",
      "     |      \n",
      "     |      This method releases resources held by the Process object.  It is\n",
      "     |      an error to call this method if the child process is still running.\n",
      "     |  \n",
      "     |  is_alive(self)\n",
      "     |      Return whether process is alive\n",
      "     |  \n",
      "     |  join(self, timeout=None)\n",
      "     |      Wait until child process terminates\n",
      "     |  \n",
      "     |  kill(self)\n",
      "     |      Terminate process; sends SIGKILL signal or uses TerminateProcess()\n",
      "     |  \n",
      "     |  run(self)\n",
      "     |      Method to be run in sub-process; can be overridden in sub-class\n",
      "     |  \n",
      "     |  start(self)\n",
      "     |      Start child process\n",
      "     |  \n",
      "     |  terminate(self)\n",
      "     |      Terminate process; sends SIGTERM signal or uses TerminateProcess()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from multiprocessing.process.BaseProcess:\n",
      "     |  \n",
      "     |  exitcode\n",
      "     |      Return exit code of process or `None` if it has yet to stop\n",
      "     |  \n",
      "     |  ident\n",
      "     |      Return identifier (PID) of process or `None` if it has yet to start\n",
      "     |  \n",
      "     |  pid\n",
      "     |      Return identifier (PID) of process or `None` if it has yet to start\n",
      "     |  \n",
      "     |  sentinel\n",
      "     |      Return a file descriptor (Unix) or handle (Windows) suitable for\n",
      "     |      waiting for process termination.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from multiprocessing.process.BaseProcess:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  authkey\n",
      "     |  \n",
      "     |  daemon\n",
      "     |      Return whether process is a daemon\n",
      "     |  \n",
      "     |  name\n",
      "    \n",
      "    class ProcessError(builtins.Exception)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProcessError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class TimeoutError(ProcessError)\n",
      "     |  Common base class for all non-exit exceptions.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimeoutError\n",
      "     |      ProcessError\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors inherited from ProcessError:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.Exception:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "\n",
      "FUNCTIONS\n",
      "    Array(typecode_or_type, size_or_initializer, *, lock=True) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a synchronized shared array\n",
      "    \n",
      "    Barrier(parties, action=None, timeout=None) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a barrier object\n",
      "    \n",
      "    BoundedSemaphore(value=1) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a bounded semaphore object\n",
      "    \n",
      "    Condition(lock=None) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a condition object\n",
      "    \n",
      "    Event() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns an event object\n",
      "    \n",
      "    JoinableQueue(maxsize=0) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a queue object\n",
      "    \n",
      "    Lock() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a non-recursive lock object\n",
      "    \n",
      "    Manager() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a manager associated with a running server process\n",
      "        \n",
      "        The managers methods such as `Lock()`, `Condition()` and `Queue()`\n",
      "        can be used to create shared objects.\n",
      "    \n",
      "    Pipe(duplex=True) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns two connection object connected by a pipe\n",
      "    \n",
      "    Pool(processes=None, initializer=None, initargs=(), maxtasksperchild=None) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a process pool object\n",
      "    \n",
      "    Queue(maxsize=0) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a queue object\n",
      "    \n",
      "    RLock() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a recursive lock object\n",
      "    \n",
      "    RawArray(typecode_or_type, size_or_initializer) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a shared array\n",
      "    \n",
      "    RawValue(typecode_or_type, *args) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a shared object\n",
      "    \n",
      "    Semaphore(value=1) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a semaphore object\n",
      "    \n",
      "    SimpleQueue() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a queue object\n",
      "    \n",
      "    Value(typecode_or_type, *args, lock=True) method of multiprocessing.context.DefaultContext instance\n",
      "        Returns a synchronized shared object\n",
      "    \n",
      "    active_children()\n",
      "        Return list of process objects corresponding to live child processes\n",
      "    \n",
      "    allow_connection_pickling() method of multiprocessing.context.DefaultContext instance\n",
      "        Install support for sending connections and sockets\n",
      "        between processes\n",
      "    \n",
      "    cpu_count() method of multiprocessing.context.DefaultContext instance\n",
      "        Returns the number of CPUs in the system\n",
      "    \n",
      "    current_process()\n",
      "        Return process object representing the current process\n",
      "    \n",
      "    freeze_support() method of multiprocessing.context.DefaultContext instance\n",
      "        Check whether this is a fake forked process in a frozen executable.\n",
      "        If so then run code specified by commandline and exit.\n",
      "    \n",
      "    get_all_start_methods() method of multiprocessing.context.DefaultContext instance\n",
      "    \n",
      "    get_context(method=None) method of multiprocessing.context.DefaultContext instance\n",
      "    \n",
      "    get_logger() method of multiprocessing.context.DefaultContext instance\n",
      "        Return package logger -- if it does not already exist then\n",
      "        it is created.\n",
      "    \n",
      "    get_start_method(allow_none=False) method of multiprocessing.context.DefaultContext instance\n",
      "    \n",
      "    log_to_stderr(level=None) method of multiprocessing.context.DefaultContext instance\n",
      "        Turn on logging and add a handler which prints to stderr\n",
      "    \n",
      "    parent_process()\n",
      "        Return process object representing the parent process\n",
      "    \n",
      "    set_executable(executable) method of multiprocessing.context.DefaultContext instance\n",
      "        Sets the path to a python.exe or pythonw.exe binary used to run\n",
      "        child processes instead of sys.executable when using the 'spawn'\n",
      "        start method.  Useful for people embedding Python.\n",
      "    \n",
      "    set_forkserver_preload(module_names) method of multiprocessing.context.DefaultContext instance\n",
      "        Set list of module names to try to load in forkserver process.\n",
      "        This is really just a hint.\n",
      "    \n",
      "    set_start_method(method, force=False) method of multiprocessing.context.DefaultContext instance\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Array', 'AuthenticationError', 'Barrier', 'BoundedSemaphor...\n",
      "\n",
      "FILE\n",
      "    /usr/lib/python3.8/multiprocessing/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695d9c3",
   "metadata": {},
   "source": [
    "One of the useful functions in multiprocessing is cpu_count(). This returns the number of CPUs (computer cores) available on your computer to be used for a parallel program. Type into ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74139ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0225b21",
   "metadata": {},
   "source": [
    "to see how many cores you have available.\n",
    "\n",
    "Nearly all modern computers have several processor cores, so you should see that you have at least 2, and perhaps as many as 40 available on your machine. Each of these cores is available to do work, in parallel, as part of your Python script. For example, if you have two cores in your computer, then your script should ideally be able to do two things at once. Equally, if you have forty cores available, then your script should ideally be able to do forty things at once.\n",
    "\n",
    "Multiprocessing allows your script to do lots of things at once by actually running multiple copies of your script in parallel, with (normally) one copy per processor core on your computer. One of these copies is known as the master copy, and is the one that is used to control all of worker copies. Because of this, multiprocessing python code has to be written into a text file and executed using the python interpreter. It is not recommended to try to run a multiprocessing python script interactively, e.g. via ipython or ipython notebook. In addition, because multiprocessing achieves parallelism by running multiple copies of your script, it forces you to write it in a particular way. All imports should be at the top of the script, followed by all function and class definitions. This is to ensure that all copies of the script have access to the same modules, functions and classes. Then, you should ensure that only the master copy of the script runs the code by protecting it behind an <code>if __name__ == \"__main__\"</code> statement.\n",
    "\n",
    "An example (non-functional) script is shown below;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d97486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports should be at the top of your script\n",
    "import multiprocessing\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# all function and class definitions must be next\n",
    "def add(x, y):\n",
    "    \"\"\"Function to return the sum of the two arguments\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def product(x, y):\n",
    "    \"\"\"Function to return the product of the two arguments\"\"\"\n",
    "    return x * y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You must now protect the code being run by\n",
    "    # the master copy of the script by placing it\n",
    "    # in this block\n",
    "\n",
    "    a = [1, 2, 3, 4, 5]\n",
    "    b = [6, 7, 8, 9, 10]\n",
    "\n",
    "    # Now write your parallel code...\n",
    "    # etc. etc.z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9f243",
   "metadata": {},
   "source": [
    "(if you are interested, take a look [here](https://chryswoods.com/parallel_python/gil.html) for more information about why parallel Python is based on forking multiple processes, rather than splitting multiple threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669b39e",
   "metadata": {},
   "source": [
    "## Part 2.2: Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8414cd4",
   "metadata": {},
   "source": [
    "One of the core multiprocessing features is multiprocessing.Pool. This provides a pool of workers that can be used to parallelise a map.\n",
    "\n",
    "For example, create a new script called pool.py and type into it;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27432b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available equals 12\n",
      "The sum of the square of the first 5000 integers is 41679167500\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Function to return the square of the argument\"\"\"\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print the number of cores\n",
    "    print(\"Number of cores available equals %s\" % cpu_count())\n",
    "\n",
    "    # create a pool of workers\n",
    "    with Pool() as pool:\n",
    "        # create an array of 5000 integers, from 1 to 5000\n",
    "        r = range(1, 5001)\n",
    "\n",
    "        result = pool.map(square, r)\n",
    "\n",
    "    total = reduce(lambda x, y: x + y, result)\n",
    "\n",
    "    print(\"The sum of the square of the first 5000 integers is %s\" % total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea905c8b",
   "metadata": {},
   "source": [
    "So how does this work? The line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a0625",
   "metadata": {},
   "source": [
    "<code>with Pool() as pool:</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867152d1",
   "metadata": {},
   "source": [
    "has created a pool of worker copies of your script, with the number of workers equalling the number of cores reported by cpu_count(). You can control the number of copies by specifying the value of processes in the constructor for Pool, e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ff53a",
   "metadata": {},
   "source": [
    "<code>with Pool(processes=5) as pool:</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cb00e2",
   "metadata": {},
   "source": [
    "The line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b92fa",
   "metadata": {},
   "source": [
    "<code>r = range(1,5001)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe60a2",
   "metadata": {},
   "source": [
    "is a quick way to create a list of 5000 integers, from 1 to 5000. The parallel work is conducted on the line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03749d3",
   "metadata": {},
   "source": [
    "<code>result = pool.map(square, r)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0de14f",
   "metadata": {},
   "source": [
    "This performs a map of the function square over the list of items of r. The map is divided up over all of the workers in the pool. This means that, if you have 10 workers (e.g. if you have 10 cores), then each worker will perform only one tenth of the work (e.g. calculating the square of 500 numbers). If you have 2 workers, then each worker will perform only half of the work (e.g. calculating the square of 2500 numbers).\n",
    "\n",
    "The next line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c929e",
   "metadata": {},
   "source": [
    "<code>total = reduce(lambda x, y: x + y, result)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537954e",
   "metadata": {},
   "source": [
    "is just a standard reduce used to sum together all of the results.\n",
    "\n",
    "You can verify that the square function is divided between your workers by using a multiprocessing.current_process().pid call, which will return the process ID (PID) of the worker process. Edit your pool.py script and set the contents equal to;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4e2a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers equals 2\n",
      "Worker 3290 calculating square of 1\n",
      "Worker 3291 calculating square of 4\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 2\n",
      "Worker 3291 calculating square of 5\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 3\n",
      "Worker 3291 calculating square of 6\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 10\n",
      "Worker 3291 calculating square of 7\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 11\n",
      "Worker 3291 calculating square of 8\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 12\n",
      "Worker 3291 calculating square of 9\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 13\n",
      "Worker 3291 calculating square of 16\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 14\n",
      "Worker 3291 calculating square of 17\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 15\n",
      "Worker 3291 calculating square of 18\n",
      "\n",
      "\n",
      "Worker 3290 calculating square of 19\n",
      "\n",
      "Worker 3290 calculating square of 20\n",
      "\n",
      "The sum of the square of the first 5000 integers is 2870\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from multiprocessing import Pool, current_process\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Function to return the square of the argument\"\"\"\n",
    "    print(\"Worker %s calculating square of %s\\n\" % (current_process().pid, x))\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nprocs = 2\n",
    "\n",
    "    # print the number of cores\n",
    "    print(\"Number of workers equals %d\" % nprocs)\n",
    "\n",
    "    # create a pool of workers\n",
    "    with Pool(processes=nprocs) as pool:\n",
    "        # create an array of 5000 integers, from 1 to 5000\n",
    "        r = range(1, 21)\n",
    "\n",
    "        result = pool.map(square, r)\n",
    "\n",
    "    total = reduce(lambda x, y: x + y, result)\n",
    "\n",
    "    print(\"The sum of the square of the first 5000 integers is %s\" % total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f2afb",
   "metadata": {},
   "source": [
    "(the exact PIDs of the workers, and the order in which they print will be different on your machine)\n",
    "\n",
    "You can see in the output that there are two workers, signified by the two different worker PIDs. The work has been divided evenly amongst them. Edit pool.py and change the value of nprocs. How is the work divided as you change the number of workers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036791a",
   "metadata": {},
   "source": [
    "### Using multiple pools in a single script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc204a",
   "metadata": {},
   "source": [
    "You can use more than one multiprocessing.Pool at a time in your script, but you should ensure that you use them one after another. The way multiprocessing.Pool works is to fork your script into the team of workers when you create a Pool object. Each worker contains a complete copy of all of the functions and variables that exist at the time of the fork. This means that any changes after the fork will not be held by the other workers, e.g. open a new python script called broken_pool.py and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9f9f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square result: [1, 4, 9, 16, 25]\n",
      "Cube result: [1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Return the square of the argument\"\"\"\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    r = [1, 2, 3, 4, 5]\n",
    "\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(square, r)\n",
    "\n",
    "        print(\"Square result: %s\" % result)\n",
    "\n",
    "        def cube(x):\n",
    "            \"\"\"Return the cube of the argument\"\"\"\n",
    "            return x * x * x\n",
    "\n",
    "        # result = pool.map(cube, r)\n",
    "        # If you run this script you should see an error such as\n",
    "        # AttributeError: Can't get attribute 'cube' on <module '__main__' from 'pool.py'>\n",
    "\n",
    "        print(\"Cube result: %s\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5435d0",
   "metadata": {},
   "source": [
    "(you may also find that your python script hangs and cannot be killed. To kill the script, hold CTRL and Z to background the task, then type kill -9 %1 to kill the python script)\n",
    "\n",
    "The problem is that pool was created before the cube function. The worker copies of the script were thus created before cube was defined, and so don’t contain a copy of this function. This is one of the reasons why you should always define your functions above the <code>if __name__ == \"__main__\"</code> block.\n",
    "\n",
    "Alternatively, if you have to define the function in the <code>__main__</code> block, then ensure that you create the pool after the definition. For example, one fix here is to create a second pool for the second map, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c9797c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square result: [1, 4, 9, 16, 25]\n",
      "Cube result: [1, 8, 27, 64, 125]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Return the square of the argument\"\"\"\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    r = [1, 2, 3, 4, 5]\n",
    "\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(square, r)\n",
    "\n",
    "        print(\"Square result: %s\" % result)\n",
    "\n",
    "    def cube(x):\n",
    "        \"\"\"Return the cube of the argument\"\"\"\n",
    "        return x * x * x\n",
    "\n",
    "    with Pool() as pool:\n",
    "        result = pool.map(cube, r)\n",
    "\n",
    "        print(\"Cube result: %s\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21db61",
   "metadata": {},
   "source": [
    "## Part 2.3: Parallel map/reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92d363",
   "metadata": {},
   "source": [
    "The multiprocessing.Pool provides an excellent mechanism for the parallelisation of map/reduce style calculations. However, there are a number of caveats that make it more difficult to use than the simple map/reduce that was introduced in [Part 1](https://chryswoods.com/parallel_python/map.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c122e",
   "metadata": {},
   "source": [
    "### Mapping functions with multiple arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08151561",
   "metadata": {},
   "source": [
    "The Pool.map function only supports mapping functions that have a single argument. This means that if you want to map over a function which expects multiple arguments you can’t use it. Instead, you can use Pool.starmap which expects you to pass it a list of tuples where each tuple will be unpacked and passed to the function.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "228bf85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 11]\n"
     ]
    }
   ],
   "source": [
    "args = [(1, 6), (2, 7), (3, 8)]\n",
    "with Pool() as pool:\n",
    "    print(pool.starmap(add, args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee33e7",
   "metadata": {},
   "source": [
    "will effectively return:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b4f51",
   "metadata": {},
   "source": [
    "<code>[add(1, 6), add(2, 7), add(3, 8)]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadb2f1",
   "metadata": {},
   "source": [
    "The above trick allows you to use any multiple-argument function. However, doing this now means that you have to convert multiple lists of arguments into a single list of multiple arguments. For example, we need to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a40b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "def add(x, y):\n",
    "    \"\"\"Return the sum of the two arguments\"\"\"\n",
    "    return x + y\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10]\n",
    "\n",
    "result = map(add, a, b)\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d6ba6",
   "metadata": {},
   "source": [
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3b584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def add(x, y):\n",
    "    \"\"\"Return the sum of the tuple of two arguments\"\"\"\n",
    "    return x+y\n",
    "\n",
    "a_b = [(1,6), (2,7), (3,8), (4,9), (5,10)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool() as pool:\n",
    "        result = pool.starmap(add, a_b)\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec012d",
   "metadata": {},
   "source": [
    "Combining the two lists of arguments into a single list of tuples could be painful. Fortunately, python provides the zip function. This automatically zips up N lists into one list of tuples (each tuple containing N items). For example, type into ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4000ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6, 11), (2, 7, 12), (3, 8, 13), (4, 9, 14), (5, 10, 15)]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10]\n",
    "c = [11, 12, 13, 14, 15]\n",
    "\n",
    "args = zip(a, b, c)\n",
    "print(list(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfacfcab",
   "metadata": {},
   "source": [
    "You thus need to use zip to zip together the arguments when you call Pool.starmap. For example, the above example should be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb18c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def add(x, y):\n",
    "    \"\"\"Return the sum of the tuple of two arguments\"\"\"\n",
    "    return x + y\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool() as pool:\n",
    "        result = pool.starmap(add, zip(a,b))\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51fd54",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d7df3",
   "metadata": {},
   "source": [
    "Edit your countlines.py script that you wrote for [Part 1](https://chryswoods.com/parallel_python/reduce_answer1.html) so that you use multiprocessing to parallelise the counting of lines. Note that you will not be able to use lambda in the pool.map function.\n",
    "\n",
    "If you get stuck or want some inspiration, a possible answer is given [here](https://chryswoods.com/parallel_python/mapreduce2_answer1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69802f10",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50828d8e",
   "metadata": {},
   "source": [
    "Below are two functions. The first counts the number of times every word in a file appears in that file, returning the result as a dictionary (key is the word, the value is the number of times it appears). The second function combines (reduces) two dictionaries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "842d1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def count_words(filename):\n",
    "    \"\"\"\n",
    "    Count the number of times every word in the file `filename`\n",
    "    is contained in this file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): the filename to count the words in\n",
    "\n",
    "    Returns:\n",
    "        dict: a mapping of word to count\n",
    "    \"\"\"\n",
    "\n",
    "    all_words = {}\n",
    "\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "\n",
    "            for word in words:\n",
    "                #lowercase the word and remove all\n",
    "                #characters that are not [a-z] or hyphen\n",
    "                word = word.lower()\n",
    "                match = re.search(r\"([a-z\\-]+)\", word)\n",
    "\n",
    "                if match:\n",
    "                    word = match.groups()[0]\n",
    "\n",
    "                    if word in all_words:\n",
    "                        all_words[word] += 1\n",
    "                    else:\n",
    "                        all_words[word] = 1\n",
    "\n",
    "    return all_words\n",
    "\n",
    "\n",
    "def reduce_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Combine (reduce) the passed two dictionaries to return\n",
    "    a dictionary that contains the keys of both, where the\n",
    "    values are equal to the sum of values for each key\n",
    "    \"\"\"\n",
    "\n",
    "    # explicitly copy the dictionary, as otherwise\n",
    "    # we risk modifying 'dict1'\n",
    "    combined = {}\n",
    "\n",
    "    for key in dict1:\n",
    "        combined[key] = dict1[key]\n",
    "\n",
    "    for key in dict2:\n",
    "        if key in combined:\n",
    "            combined[key] += dict2[key]\n",
    "        else:\n",
    "            combined[key] = dict2[key]\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e934f",
   "metadata": {},
   "source": [
    "Use the above two function to write a parallel Python script called countwords.py that counts how many times each word used by Shakespeare appears in all of his plays, e.g. by using the command line call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2f3db",
   "metadata": {},
   "source": [
    "<code>python countwords.py shakespeare/*</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d01041",
   "metadata": {},
   "source": [
    "Have your script print out every word that appears more than 2000 times across all of the plays. The words should be printed out in alphabetical order, and printed together with the number of times that they are used.\n",
    "\n",
    "If you get stuck or want some inspiration, a possible answer is given [here](https://chryswoods.com/parallel_python/mapreduce2_answer2.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
